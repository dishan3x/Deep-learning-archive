{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_1_linear_regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOaHaoqCAUDp",
        "colab_type": "text"
      },
      "source": [
        "# <i>Assignment 1  - Deeplearning 890 </i> \n",
        "\n",
        "Problem 1\n",
        "\n",
        "First problem of assignment is about training a linear regression model to predic housing data submitted by [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html). In this example linear regression model is created using pytorch library. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GzHF46Syj4_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#<libraries>\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import normalize\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler \n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import skimage.io as io"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlugtDzGaU0a",
        "colab_type": "text"
      },
      "source": [
        "loading data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJvZw-WiuHn-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading housing \n",
        "from sklearn.datasets import load_boston\n",
        "data, targets = load_boston(return_X_y=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbuD5vD-uQ8w",
        "colab_type": "text"
      },
      "source": [
        "Process data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sPx-n21zpnX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#splitting data\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, targets, test_size=0.3 , random_state=5)\n",
        "\n",
        "# normalize data\n",
        "\n",
        "# setting the scaler\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_train)\n",
        "\n",
        "# Transform train and test data according to the scaler \n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert to floar\n",
        "X_train = np.array(X_train, dtype=np.float32)\n",
        "X_test = np.array(X_test, dtype=np.float32)\n",
        "y_train = np.array(y_train, dtype=np.float32)\n",
        "y_test = np.array(y_test, dtype=np.float32)\n",
        "\n",
        "\n",
        "# rearrange for one dimention\n",
        "y_train = torch.tensor(y_train, dtype=torch.float).view(-1, 1).cuda()\n",
        "y_test = torch.tensor(y_test, dtype=torch.float).view(-1, 1).cuda()\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "401D2oov58Um",
        "colab_type": "text"
      },
      "source": [
        "Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH3D0d0cWJo7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class linearRegression(torch.nn.Module):\n",
        "    def __init__(self, inputSize, outputSize):\n",
        "        super(linearRegression, self).__init__()\n",
        "        self.linear = torch.nn.Linear(inputSize, outputSize)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDr6K0zz6GZ5",
        "colab_type": "text"
      },
      "source": [
        "Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oa00P48PZ3Xm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### For GPU #######\n",
        "\n",
        "# Create the model\n",
        "inputDim = 13      # takes variable 'x' \n",
        "outputDim = 1       # takes variable 'y'\n",
        "model = linearRegression(inputDim, outputDim)\n",
        "\n",
        "# Training  parameters \n",
        "learningRate = 0.001\n",
        "epochs = 500\n",
        "\n",
        "# Store information for each epoch \n",
        "loss_arr    = [] \n",
        "val_arr     = []\n",
        "epoch_arr   = []\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "\n",
        "criterion = torch.nn.MSELoss()  # mean squred error\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Converting inputs and labels to Variable\n",
        "    if torch.cuda.is_available():\n",
        "        X_train_input = Variable(torch.from_numpy(X_train).cuda())\n",
        "        X_test_input = Variable(torch.from_numpy(X_test).cuda())\n",
        "    else:\n",
        "        X_train_input = Variable(torch.from_numpy(X_train))\n",
        "        y_train_labels = Variable(torch.from_numpy(y_train))\n",
        "\n",
        "\n",
        "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # get output from the model, given the inputs\n",
        "    # Predict the output\n",
        "    pred  = model(X_train_input) \n",
        "    \n",
        "    # get loss for the predicted output\n",
        "    loss = criterion(pred, y_train)\n",
        "\n",
        "    # get gradients w.r.t to parameters\n",
        "    loss.backward()\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # Validate model\n",
        "    model.eval() # Put the model into evaluation mode\n",
        "\n",
        "    with torch.no_grad():\n",
        "      val_pred = model(X_test_input)\n",
        "      val_loss = criterion(val_pred, y_test) # Calculate validation loss\n",
        "      loss_arr.append(loss.item())\n",
        "      val_arr.append(val_loss.item())\n",
        "      epoch_arr.append(epoch+1)\n",
        "      print(f\"Epoch: {epoch} Learning rate:{learningRate} Training Loss: {loss.item()} Test Loss: {val_loss.item()}\")\n",
        "\n",
        "  \n",
        "\n",
        "print(\"Training complete\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvfnPk2yseuC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure()\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.title('Performance with a learning rate of {}'.format(learningRate))\n",
        "plt.plot(epoch_arr,loss_arr,label='Train loss')\n",
        "\n",
        "plt.plot(epoch_arr,val_arr,label='Validation loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Mean Square Error [$medv^2$]')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfNkOYGO0VDC",
        "colab_type": "text"
      },
      "source": [
        "# Results\n",
        "\n",
        "![picture](https://drive.google.com/file/d/1M-PsPmTQmOlqRIYJujZcvop0ye3IlFqP/view?usp=sharing)\n",
        "Epoch: 499 Learning rate:0.001 Training Loss: 85.93895721435547 Test Loss: 94.96333312988281\n",
        "\n",
        "\n",
        "![picture](https://drive.google.com/file/d/1fE_sXNBKA6I33JbAoA7HfdSC8pCsk9-N/view?usp=sharing)\n",
        "Epoch: 199 Learning rate:0.01 Training Loss: 51.73222351074219 Test Loss: 65.39244079589844\n",
        "\n",
        "\n",
        "![picture](https://drive.google.com/file/d/1m3gbQwjJu5WxwS1PR60XB0TYPqnMw7hn/view?usp=sharing)\n",
        "Epoch: 199 Learning rate:0.1 Training Loss: 23.646242141723633 Test Loss: 36.42483139038086\n",
        "\n",
        "Iterations needed for covergence  \n",
        "* 0.001 - 300 epochs\n",
        "* 0.01  - 80 epochs\n",
        "* 0.1   - 30 epochs\n",
        "\n",
        "# Discussion\n",
        "\n",
        "According to the training information the best and lowest validation loss was given by the learning rate of 0.1. All the learning rate do converges having  higher validation losses over train loss.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYs8SZY6Ahhg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Please follow the links to view the images."
      ],
      "execution_count": 1,
      "outputs": []
    }
  ]
}